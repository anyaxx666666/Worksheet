{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9IK9UKY6Bxcm"
      },
      "source": [
        "# Cost Function and Gradient Descent"
      ],
      "id": "9IK9UKY6Bxcm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ebVP9hcQBxcn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math, copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "ebVP9hcQBxcn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UliaUCJnBxco"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "You would like a model which can predict housing prices given the size of the house.  \n",
        "Let's use the following data\n",
        "\n",
        "\n",
        "| Size (1000 sqft)     | Price (INR in Lakhs) |\n",
        "| -------------------| ------------------------ |\n",
        "| 1                 | 30                      |\n",
        "| 2                  | 50                      |\n",
        "| 3                  | 68                      |\n",
        "| 4                  | 75                      |\n",
        "| 5                  | 100                      |"
      ],
      "id": "UliaUCJnBxco"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oBFELuzABxco"
      },
      "outputs": [],
      "source": [
        "x_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0])           #(size in 1000s square feet)\n",
        "y_train = np.array([30.0, 50.0, 68.0, 75.0, 100.0])           #(price in INR lakhs)"
      ],
      "id": "oBFELuzABxco"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "o1YrmOiCBxcp"
      },
      "source": [
        "## Computing Cost\n",
        "The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.\n",
        "\n",
        "The equation for cost with one variable is:\n",
        "  $$J(θ_0, θ_1) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (h(x^{(i)}) - y^{(i)})^2 \\tag{1}$$\n",
        "\n",
        "where\n",
        "  $$h(x^{(i)}) = θ_0 + θ_1x^{(i)} \\tag{2}$$\n",
        "  \n",
        "- $h(x^{(i)})$ is our prediction for example $i$ using parameters $θ_0, θ_1$.  \n",
        "- $(h(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   \n",
        "- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(θ_0, θ_1)$. This cost is mean squared error (MSE). Dividing by m and 2m is the same. 2m just yields smaller numbers for easier computations.\n",
        "\n",
        "- Sometimes, $θ_0$ is also referred as Bias(b) and $θ_1$ onwards as Weights(w). We will use b, w in following sections.\n"
      ],
      "id": "o1YrmOiCBxcp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "X4kyG4sbBxcp"
      },
      "source": [
        "The code below calculates cost by looping over each example. In each loop:\n",
        "- `h_x`, a prediction is calculated\n",
        "- the difference between the target and the prediction is calculated and squared.\n",
        "- this is added to the total cost."
      ],
      "id": "X4kyG4sbBxcp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hEB4eR2JBxcp"
      },
      "outputs": [],
      "source": [
        "def compute_cost(x, y, b, w):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      b, w (scalar)    : theta_0 and theta_1, model parameters\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using theta_0, theta_1 as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = x.shape[0]\n",
        "\n",
        "    cost_sum = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        # Calculate h(x) and then calculate the cost from y\n",
        "        # YOUR CODE HERE (find h_x then use it to find cost):\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################\n",
        "        cost_sum = cost_sum + cost\n",
        "\n",
        "    total_cost = (1 / (2*m)) * cost_sum\n",
        "\n",
        "    return total_cost"
      ],
      "id": "hEB4eR2JBxcp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.1\"></a>\n",
        "## Gradient descent summary\n",
        "So far in this course, you have developed a linear model that predicts $h(x^{(i)})$:\n",
        "$$h(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
        "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $h(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
        "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (h(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
      ],
      "metadata": {
        "id": "sX_c0ZXoFgXp"
      },
      "id": "sX_c0ZXoFgXp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`Gradient descent` is as follows:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
        "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline\n",
        " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "where, parameters $w$, $b$ are updated simultaneously.  \n",
        "To make it easier for you, the paritial derivatives are calculated:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (h(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
        "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (h(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Here *simultaneously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
      ],
      "metadata": {
        "id": "drgQ3NugFo9k"
      },
      "id": "drgQ3NugFo9k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.2\"></a>\n",
        "## Implement Gradient Descent\n",
        "You will implement gradient descent algorithm for one feature. You will need three functions.\n",
        "- `compute_gradient` has been implemented using equation (4) and (5) above\n",
        "- `compute_cost` implementing equation (2) above, which you have completed\n",
        "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
        "\n",
        "Conventions:\n",
        "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
        "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
      ],
      "metadata": {
        "id": "EA5eHCk-F5XE"
      },
      "id": "EA5eHCk-F5XE"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples\n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters\n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
        "     \"\"\"\n",
        "\n",
        "    # Number of training examples\n",
        "    m = x.shape[0]\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        h_x = w * x[i] + b\n",
        "        dj_dw_i = (h_x - y[i]) * x[i]\n",
        "        dj_db_i = h_x - y[i]\n",
        "        dj_db += dj_db_i\n",
        "        dj_dw += dj_dw_i\n",
        "\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "v9866DepB-lF"
      },
      "id": "v9866DepB-lF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_40291_2.5\"></a>\n",
        "###  Gradient Descent\n",
        "Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in `gradient_descent`. The details of the implementation are described in the comments. Below, you will utilize this function to find optimal values of $w$ and $b$ on the training data."
      ],
      "metadata": {
        "id": "8MnufVdXHYrw"
      },
      "id": "8MnufVdXHYrw"
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,))  : Data, m examples\n",
        "      y (ndarray (m,))  : target values\n",
        "      w_in,b_in (scalar): initial values of model parameters\n",
        "      alpha (float):     Learning rate\n",
        "      num_iters (int):   number of iterations to run gradient descent\n",
        "      cost_function:     function to call to produce cost\n",
        "      gradient_function: function to call to produce gradient\n",
        "\n",
        "    Returns:\n",
        "      w (scalar): Updated value of parameter after running gradient descent\n",
        "      b (scalar): Updated value of parameter after running gradient descent\n",
        "      J_history (List): History of cost values\n",
        "      p_history (list): History of parameters [w,b]\n",
        "      \"\"\"\n",
        "\n",
        "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    p_history = []\n",
        "    b = b_in\n",
        "    w = w_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters using gradient_function\n",
        "        dj_dw, dj_db = #  YOUR CODE HERE\n",
        "\n",
        "        # Update Parameters using equation (3) above\n",
        "        # YOUR CODE HERE (2 steps)\n",
        "\n",
        "\n",
        "\n",
        "        # Save cost J and parameters at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            # YOUR CODE HERE (2 steps). Hint: append the values\n",
        "\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
        "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
        "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        "\n",
        "    return w, b, J_history, p_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "aTGSxevdHZwB"
      },
      "id": "aTGSxevdHZwB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "w_init = 0\n",
        "b_init = 0\n",
        "# some gradient descent settings\n",
        "iterations = 10000\n",
        "tmp_alpha = 1.0e-2\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient)\n",
        "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
      ],
      "metadata": {
        "id": "kKuxzTXNHdi4"
      },
      "id": "kKuxzTXNHdi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost versus iterations of gradient descent\n",
        "A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step."
      ],
      "metadata": {
        "id": "pz5miBGBHiAD"
      },
      "id": "pz5miBGBHiAD"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot cost versus iteration\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
        "ax1.plot(J_hist[:100])\n",
        "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
        "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
        "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
        "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ktHTalaZGPc5"
      },
      "id": "ktHTalaZGPc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions\n",
        "Now that you have discovered the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. You will notice the predicted values are nearly the same as the training values for the same housing, and predictions for newer values also are inline with the training data."
      ],
      "metadata": {
        "id": "_VQ1JGsMHnPv"
      },
      "id": "_VQ1JGsMHnPv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the hypothesis equation and parameters obtained. Get predictions for 1000 sqft, 2000sqft and 1750 sqft.\n",
        "# Note: your model is trained on per thousand data, so 1 = 1000\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "\n",
        "\n",
        "\n",
        "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand INR\")\n",
        "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand INR\")\n",
        "print(f\"1750 sqft house prediction {w_final*1.75 + b_final:0.1f} Thousand INR\")\n",
        "\n",
        "\n",
        "\n",
        "# Expected:\n",
        "# For 1000 = 31.6 Thousand INR\n",
        "# For 2000 = 48.1 Thousand INR\n",
        "# For 1750 = 44.0 Thousand INR"
      ],
      "metadata": {
        "id": "8FEPlB5PHpcm"
      },
      "id": "8FEPlB5PHpcm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-pxne2qIWIg"
      },
      "id": "S-pxne2qIWIg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc-autonumbering": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}